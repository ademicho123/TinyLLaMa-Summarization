{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates the process of training a text summarization model using the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ELITEBOOK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Import custom modules\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data, preprocess_data, save_data\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_model, train_model, save_model, summarize\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_rouge, calculate_bleu\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualizations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_loss\n",
      "File \u001b[1;32mc:\\Users\\ELITEBOOK\\OneDrive\\Desktop\\Projects\\TinyLLaMa-Summarization\\src\\model.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mL\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightning'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = str(Path.cwd().parent) if 'notebooks' in str(Path.cwd()) else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import custom modules\n",
    "from src.preprocessing import load_data, preprocess_data, save_data\n",
    "from src.model import initialize_model, train_model, save_model, summarize\n",
    "from src.evaluation import calculate_rouge, calculate_bleu\n",
    "from src.visualizations import plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "raw_data_dir = r\"C:\\Users\\ELITEBOOK\\OneDrive\\Desktop\\Projects\\TinyLLaMa-Summarization\\data\\raw\"\n",
    "processed_data_dir = r\"C:\\Users\\ELITEBOOK\\OneDrive\\Desktop\\Projects\\TinyLLaMa-Summarization\\data\\processed\"\n",
    "\n",
    "# Sample size\n",
    "SAMPLE_SIZE = 0.10\n",
    "\n",
    "files = [\"train.csv\", \"test.csv\", \"validation.csv\"]\n",
    "for file in files:\n",
    "    data = load_data(os.path.join(raw_data_dir, file))\n",
    "    sampled_data = data.sample(frac=SAMPLE_SIZE, random_state=42)\n",
    "    articles, highlights = preprocess_data(sampled_data)\n",
    "    save_data(articles, highlights, processed_data_dir, f\"processed_{file}\")\n",
    "    print(f\"Processed {len(articles)} samples from {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "processed_data_dir = r\"C:\\Users\\ELITEBOOK\\OneDrive\\Desktop\\Projects\\TinyLLaMa-Summarization\\data\\processed\"\n",
    "\n",
    "# Load processed training data\n",
    "train_df = pd.read_csv(os.path.join(processed_data_dir, \"processed_train.csv\"))\n",
    "train_data = [\n",
    "    {'article': row['article'], 'highlights': row['highlights']} \n",
    "    for _, row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model, tokenizer = initialize_model(model_name)\n",
    "\n",
    "# Train model\n",
    "training_stats = train_model(model, tokenizer, train_data, epochs=5)\n",
    "\n",
    "# Plot training loss\n",
    "plot_loss(training_stats['losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "with open(os.path.join(processed_data_dir, \"processed_test.csv\"), \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Evaluate on sample from test set\n",
    "print(\"\\nEvaluating model on sample test data:\")\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "for idx, item in enumerate(test_data[:5]):\n",
    "    generated_summary = summarize(model, tokenizer, item['article'])\n",
    "    rouge = calculate_rouge(item['highlights'], generated_summary)\n",
    "    bleu = calculate_bleu(item['highlights'], generated_summary)\n",
    "    \n",
    "    rouge_scores.append(rouge)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "# Results dictionary\n",
    "results = {\n",
    "    'average_scores': {\n",
    "        'rouge1': avg_rouge1,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL,\n",
    "        'bleu': avg_bleu\n",
    "    },\n",
    "    'example_predictions': [\n",
    "        {\n",
    "            'article': item['article'],\n",
    "            'reference_summary': item['highlights'],\n",
    "            'generated_summary': summarizer.summarize(item['article']),\n",
    "        }\n",
    "        for item in test_data[:5]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dictionary\n",
    "results = {\n",
    "    'average_scores': {\n",
    "        'rouge1': avg_rouge1,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL,\n",
    "        'bleu': avg_bleu\n",
    "    },\n",
    "    'example_predictions': [\n",
    "        {\n",
    "            'article': item['article'],\n",
    "            'reference_summary': item['summary'],\n",
    "            'generated_summary': summarizer.summarize(item['article']),\n",
    "        }\n",
    "        for item in test_data[:5]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save results\n",
    "os.makedirs('../reports', exist_ok=True)\n",
    "with open('../reports/evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation results saved to reports/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "print(\"Saving model...\")\n",
    "save_model(model, tokenizer, \"../models/tiny-llama-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
