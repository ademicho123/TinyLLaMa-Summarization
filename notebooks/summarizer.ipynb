{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates the process of training a text summarization model using the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1.post300)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.11/site-packages (0.110.3)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.11/site-packages (0.30.6)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.11/site-packages (from fastapi) (0.37.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from fastapi) (2.7.3)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.11/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from starlette<0.38.0,>=0.37.2->fastapi) (4.6.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers scikit-learn rouge-score nltk fastapi uvicorn matplotlib pandas numpy seaborn emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = str(Path.cwd().parent) if 'notebooks' in str(Path.cwd()) else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import custom modules\n",
    "from src.preprocessing import load_data, preprocess_data, save_data, preprocess_batch\n",
    "from src.model import initialize_model, train_model, save_model, summarize\n",
    "from src.evaluation import calculate_rouge, calculate_bleu\n",
    "from src.visualizations import plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw\n",
      "Processed Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed\n",
      "Looking for file: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw/train.csv\n",
      "Saving processed data to: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed/processed_train.csv\n",
      "Processed 28711 samples from train.csv\n",
      "Looking for file: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw/test.csv\n",
      "Saving processed data to: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed/processed_test.csv\n",
      "Processed 1149 samples from test.csv\n",
      "Looking for file: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw/validation.csv\n",
      "Saving processed data to: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed/processed_validation.csv\n",
      "Processed 1337 samples from validation.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  \n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Raw Data Path:\", raw_data_dir)\n",
    "print(\"Processed Data Path:\", processed_data_dir)\n",
    "\n",
    "# Sample size\n",
    "SAMPLE_SIZE = 0.10\n",
    "\n",
    "files = [\"train.csv\", \"test.csv\", \"validation.csv\"]\n",
    "for file in files:\n",
    "    file_path = os.path.join(raw_data_dir, file)\n",
    "    \n",
    "    # Debugging: Print expected file path\n",
    "    print(f\"Looking for file: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    data = load_data(file_path)  # Now it should not fail\n",
    "    sampled_data = data.sample(frac=SAMPLE_SIZE, random_state=42)\n",
    "    articles, highlights = preprocess_data(sampled_data)\n",
    "    save_data(articles, highlights, processed_data_dir, f\"processed_{file}\")\n",
    "    print(f\"Processed {len(articles)} samples from {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPU Status Check ===\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.0\n",
      "Current CUDA device: 0\n",
      "Device name: Tesla T4\n",
      "Total memory: 14917.69 MB\n",
      "GPU Compute Capability: 7.5\n",
      "nvidia-smi error: name 'nvidia_smi' is not defined\n",
      "\n",
      "=== Environment Variables ===\n",
      "CUDA_VISIBLE_DEVICES: 0\n",
      "NVIDIA_VISIBLE_DEVICES: all\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def check_gpu_status():\n",
    "    print(\"\\n=== GPU Status Check ===\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")\n",
    "    \n",
    "    # Check current device\n",
    "    if torch.cuda.is_available():\n",
    "        current_device = torch.cuda.current_device()\n",
    "        print(f\"Current CUDA device: {current_device}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "        \n",
    "        # Initialize CUDA device if not already done\n",
    "        torch.cuda.init()\n",
    "        \n",
    "        # Get device properties\n",
    "        device_props = torch.cuda.get_device_properties(current_device)\n",
    "        print(f\"Total memory: {device_props.total_memory / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU Compute Capability: {device_props.major}.{device_props.minor}\")\n",
    "    \n",
    "    # Try nvidia-smi\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"\\nNVIDIA-SMI Memory Info:\")\n",
    "        print(f\"Total: {info.total / 1024**2:.2f} MB\")\n",
    "        print(f\"Free: {info.free / 1024**2:.2f} MB\")\n",
    "        print(f\"Used: {info.used / 1024**2:.2f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"nvidia-smi error: {e}\")\n",
    "    \n",
    "    print(\"\\n=== Environment Variables ===\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "    print(f\"NVIDIA_VISIBLE_DEVICES: {os.environ.get('NVIDIA_VISIBLE_DEVICES', 'Not set')}\")\n",
    "\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 10 23:26:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   23C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use first GPU\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"  # Limit memory splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "property 'device' of 'SummarizationModel' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m initialize_model(model_name)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m training_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m plot_loss(training_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/TinyLLaMa-Summarization/src/model.py:112\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_data, batch_size, epochs)\u001b[0m\n\u001b[1;32m    102\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TextDataset(train_data, AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    104\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    105\u001b[0m     dataset, \n\u001b[1;32m    106\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Add this for faster data transfer to GPU\u001b[39;00m\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSummarizationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    114\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    115\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    116\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    124\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Add GPU warmup\u001b[39;00m\n",
      "File \u001b[0;32m~/TinyLLaMa-Summarization/src/model.py:59\u001b[0m, in \u001b[0;36mSummarizationModel.__init__\u001b[0;34m(self, model_name, lr)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Explicitly set device\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Load model to specific device\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1768\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1768\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'device' of 'SummarizationModel' object has no setter"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Free memory\n",
    "def free_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Call before training\n",
    "free_memory()\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Processed Data Path:\", processed_data_dir)\n",
    "\n",
    "# Load processed training data\n",
    "train_df = pd.read_csv(os.path.join(processed_data_dir, \"processed_train.csv\"))\n",
    "train_data = [\n",
    "    {'article': row['article'], 'highlights': row['highlights']}\n",
    "    for _, row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model, tokenizer = initialize_model(model_name)\n",
    "\n",
    "# Train model\n",
    "training_stats = train_model(train_data, epochs=5)\n",
    "\n",
    "# Plot training loss\n",
    "plot_loss(training_stats['losses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed\n",
      "Loading test data...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(test_data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 18\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Evaluate on sample from test set\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating model on sample test data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Processed Data Path:\", processed_data_dir)\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data_path = os.path.join(processed_data_dir, \"processed_test.csv\")\n",
    "\n",
    "if not os.path.exists(test_data_path):\n",
    "    raise FileNotFoundError(f\"File not found: {test_data_path}\")\n",
    "\n",
    "with open(test_data_path, \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Evaluate on sample from test set\n",
    "print(\"\\nEvaluating model on sample test data:\")\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "for idx, item in enumerate(test_data[:5]):\n",
    "    generated_summary = summarize(model, tokenizer, item['article'])\n",
    "    rouge = calculate_rouge(item['highlights'], generated_summary)\n",
    "    bleu = calculate_bleu(item['highlights'], generated_summary)\n",
    "    \n",
    "    rouge_scores.append(rouge)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "# Compute average scores\n",
    "avg_rouge1 = sum(score['rouge1'] for score in rouge_scores) / len(rouge_scores)\n",
    "avg_rouge2 = sum(score['rouge2'] for score in rouge_scores) / len(rouge_scores)\n",
    "avg_rougeL = sum(score['rougeL'] for score in rouge_scores) / len(rouge_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Results dictionary\n",
    "results = {\n",
    "    'average_scores': {\n",
    "        'rouge1': avg_rouge1,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL,\n",
    "        'bleu': avg_bleu\n",
    "    },\n",
    "    'example_predictions': [\n",
    "        {\n",
    "            'article': item['article'],\n",
    "            'reference_summary': item['highlights'],\n",
    "            'generated_summary': summarize(model, tokenizer, item['article']),\n",
    "        }\n",
    "        for item in test_data[:5]\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Evaluation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "reports_dir = os.path.join(project_root, \"reports\")\n",
    "results_path = os.path.join(reports_dir, \"evaluation_results.json\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# Create results dictionary\n",
    "results = {\n",
    "    'average_scores': {\n",
    "        'rouge1': avg_rouge1,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL,\n",
    "        'bleu': avg_bleu\n",
    "    },\n",
    "    'example_predictions': [\n",
    "        {\n",
    "            'article': item['article'],\n",
    "            'reference_summary': item['summary'],\n",
    "            'generated_summary': summarize(model, tokenizer, item['article']),\n",
    "        }\n",
    "        for item in test_data[:5]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation results saved to {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "models_dir = os.path.join(project_root, \"models\", \"tiny-llama-model\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "print(f\"Saving model to {models_dir}...\")\n",
    "save_model(model, tokenizer, models_dir)\n",
    "\n",
    "print(f\"Model saved successfully at {models_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
