{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization Model Training\n",
    "\n",
    "This notebook demonstrates the process of training a text summarization model using the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1.post300)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.2)\n",
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.11/site-packages (0.110.3)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.11/site-packages (0.30.6)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.11/site-packages (2.14.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.11/site-packages (from fastapi) (0.37.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.11/site-packages (from fastapi) (2.7.3)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.11/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from starlette<0.38.0,>=0.37.2->fastapi) (4.6.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers scikit-learn rouge-score nltk fastapi uvicorn matplotlib pandas numpy seaborn emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sagemaker-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sagemaker-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "2025-02-10 21:36:05.106165: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-10 21:36:05.120608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-10 21:36:05.138516: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-10 21:36:05.144113: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 21:36:05.157267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = str(Path.cwd().parent) if 'notebooks' in str(Path.cwd()) else str(Path.cwd())\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import custom modules\n",
    "from src.preprocessing import load_data, preprocess_data, save_data, preprocess_batch\n",
    "from src.model import initialize_model, train_model, save_model, summarize\n",
    "from src.evaluation import calculate_rouge, calculate_bleu\n",
    "from src.visualizations import plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw\n",
      "Processed Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed\n",
      "Looking for file: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw/train.csv\n",
      "Saving processed data to: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed/processed_train.csv\n",
      "Processed 28711 samples from train.csv\n",
      "Looking for file: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw/test.csv\n",
      "Saving processed data to: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed/processed_test.csv\n",
      "Processed 1149 samples from test.csv\n",
      "Looking for file: /home/sagemaker-user/TinyLLaMa-Summarization/data/raw/validation.csv\n",
      "Saving processed data to: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed/processed_validation.csv\n",
      "Processed 1337 samples from validation.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"raw\")\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Raw Data Path:\", raw_data_dir)\n",
    "print(\"Processed Data Path:\", processed_data_dir)\n",
    "\n",
    "# Sample size\n",
    "SAMPLE_SIZE = 0.10\n",
    "\n",
    "files = [\"train.csv\", \"test.csv\", \"validation.csv\"]\n",
    "for file in files:\n",
    "    file_path = os.path.join(raw_data_dir, file)\n",
    "    \n",
    "    # Debugging: Print expected file path\n",
    "    print(f\"Looking for file: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    data = load_data(file_path)  # Now it should not fail\n",
    "    sampled_data = data.sample(frac=SAMPLE_SIZE, random_state=42)\n",
    "    articles, highlights = preprocess_data(sampled_data)\n",
    "    save_data(articles, highlights, processed_data_dir, f\"processed_{file}\")\n",
    "    print(f\"Processed {len(articles)} samples from {file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Path: /home/sagemaker-user/TinyLLaMa-Summarization/data/processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m initialize_model(model_name)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m training_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Plot training loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m plot_loss(training_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/TinyLLaMa-Summarization/src/model.py:46\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_data, batch_size, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¹ Debug: Checking first training sample...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(train_data[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Print first training example\u001b[39;00m\n",
      "File \u001b[0;32m~/TinyLLaMa-Summarization/src/model.py:46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 46\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¹ Debug: Checking first training sample...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(train_data[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Print first training example\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_batch' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Processed Data Path:\", processed_data_dir)\n",
    "\n",
    "# Load processed training data\n",
    "train_df = pd.read_csv(os.path.join(processed_data_dir, \"processed_train.csv\"))\n",
    "train_data = [\n",
    "    {'article': row['article'], 'highlights': row['highlights']} \n",
    "    for _, row in train_df.iterrows()\n",
    "]\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model, tokenizer = initialize_model(model_name)\n",
    "\n",
    "# Train model\n",
    "training_stats = train_model(train_data, epochs=5)\n",
    "\n",
    "# Plot training loss\n",
    "plot_loss(training_stats['losses'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "processed_data_dir = os.path.join(project_root, \"data\", \"processed\")\n",
    "\n",
    "print(\"Processed Data Path:\", processed_data_dir)\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data_path = os.path.join(processed_data_dir, \"processed_test.csv\")\n",
    "\n",
    "if not os.path.exists(test_data_path):\n",
    "    raise FileNotFoundError(f\"File not found: {test_data_path}\")\n",
    "\n",
    "with open(test_data_path, \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Evaluate on sample from test set\n",
    "print(\"\\nEvaluating model on sample test data:\")\n",
    "rouge_scores = []\n",
    "bleu_scores = []\n",
    "\n",
    "for idx, item in enumerate(test_data[:5]):\n",
    "    generated_summary = summarize(model, tokenizer, item['article'])\n",
    "    rouge = calculate_rouge(item['highlights'], generated_summary)\n",
    "    bleu = calculate_bleu(item['highlights'], generated_summary)\n",
    "    \n",
    "    rouge_scores.append(rouge)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "# Compute average scores\n",
    "avg_rouge1 = sum(score['rouge1'] for score in rouge_scores) / len(rouge_scores)\n",
    "avg_rouge2 = sum(score['rouge2'] for score in rouge_scores) / len(rouge_scores)\n",
    "avg_rougeL = sum(score['rougeL'] for score in rouge_scores) / len(rouge_scores)\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Results dictionary\n",
    "results = {\n",
    "    'average_scores': {\n",
    "        'rouge1': avg_rouge1,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL,\n",
    "        'bleu': avg_bleu\n",
    "    },\n",
    "    'example_predictions': [\n",
    "        {\n",
    "            'article': item['article'],\n",
    "            'reference_summary': item['highlights'],\n",
    "            'generated_summary': summarize(model, tokenizer, item['article']),\n",
    "        }\n",
    "        for item in test_data[:5]\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Evaluation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "reports_dir = os.path.join(project_root, \"reports\")\n",
    "results_path = os.path.join(reports_dir, \"evaluation_results.json\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# Create results dictionary\n",
    "results = {\n",
    "    'average_scores': {\n",
    "        'rouge1': avg_rouge1,\n",
    "        'rouge2': avg_rouge2,\n",
    "        'rougeL': avg_rougeL,\n",
    "        'bleu': avg_bleu\n",
    "    },\n",
    "    'example_predictions': [\n",
    "        {\n",
    "            'article': item['article'],\n",
    "            'reference_summary': item['summary'],\n",
    "            'generated_summary': summarize(model, tokenizer, item['article']),\n",
    "        }\n",
    "        for item in test_data[:5]\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save results\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Evaluation results saved to {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to the root project directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go one level up\n",
    "models_dir = os.path.join(project_root, \"models\", \"tiny-llama-model\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "print(f\"Saving model to {models_dir}...\")\n",
    "save_model(model, tokenizer, models_dir)\n",
    "\n",
    "print(f\"Model saved successfully at {models_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
